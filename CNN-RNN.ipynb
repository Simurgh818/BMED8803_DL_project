{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fdc0796-8b23-4080-ac56-b002939a50a4",
   "metadata": {},
   "source": [
    "# This notebook uses a Convolutional Neural Net (CNN) to predict Grid Cell alignment in real time using VR trajectory as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae29d163-982d-4602-97ae-7f87b1839b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76518236-55eb-40c7-9589-71378791f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ce7c9-f015-4db1-92d4-468679f66be7",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess the input NIFTI images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47eb1e7-90a7-4ce7-8ae6-1eadfb15bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base paths and subject IDs\n",
    "base_data_dir = r'C:\\Users\\sdabiri\\OneDrive - Georgia Institute of Technology\\BMED 8803 - Stat ML for Neural data\\Project\\preprocessed'\n",
    "base_behavioral_dir = r'C:\\Users\\sdabiri\\OneDrive - Georgia Institute of Technology\\BMED 8803 - Stat ML for Neural data\\Project\\Small_Dataset'\n",
    "\n",
    "subjects = ['s05', 's14']  # List of subjects to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63d6f5d-57b0-4755-b5b0-0f647597c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths and parameters\n",
    "# data_dir = r'C:\\Users\\sdabiri\\OneDrive - Georgia Institute of Technology\\BMED 8803 - Stat ML for Neural data\\Project\\preprocessed\\s05'\n",
    "# behavioral_dir = r'C:\\Users\\sdabiri\\OneDrive - Georgia Institute of Technology\\BMED 8803 - Stat ML for Neural data\\Project\\Small_Dataset\\s05\\BehavioralData_s05'\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, subjects, base_data_dir, base_behavioral_dir, time_interval):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Loop through each subject\n",
    "        for subject in subjects:\n",
    "            # Construct data and behavioral paths\n",
    "            data_dir = os.path.join(base_data_dir, subject)\n",
    "            behavioral_dir = os.path.join(base_behavioral_dir, subject, f'BehavioralData_{subject}')\n",
    "            run_dirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) if d.startswith('run')]\n",
    "\n",
    "            if not run_dirs:\n",
    "                print(f\"No runs found for subject {subject} in {data_dir}\")\n",
    "                continue\n",
    "\n",
    "            for run_dir in run_dirs:\n",
    "                try:\n",
    "                    # Load images and labels\n",
    "                    nii_files = sorted(os.listdir(os.path.join(run_dir, 'masked_outputs_rightEC')))\n",
    "                    run_images = [nib.load(os.path.join(run_dir, 'masked_outputs_rightEC', f)).get_fdata() for f in nii_files]\n",
    "                    run_base = os.path.basename(run_dir).split('_')[0]\n",
    "\n",
    "                    # Match behavioral file\n",
    "                    search_pattern = os.path.join(behavioral_dir, f\"*{run_base}*.tsv\")\n",
    "                    behavioral_files = glob.glob(search_pattern)\n",
    "                    if not behavioral_files:\n",
    "                        print(f\"No behavioral files found for run {run_base}\")\n",
    "                        continue\n",
    "\n",
    "                    behavioral_file = behavioral_files[0]\n",
    "                    run_behavioral_data = pd.read_csv(behavioral_file, sep='\\t')\n",
    "\n",
    "                    # Synchronize images and labels\n",
    "                    orientations = run_behavioral_data['Orientation'].values\n",
    "                    timestamps = run_behavioral_data['Time'].values\n",
    "                    time_points = np.arange(0, time_interval * len(run_images), time_interval)\n",
    "                    labels = np.interp(time_points, timestamps, orientations)\n",
    "\n",
    "                    if not run_images or not labels.any():\n",
    "                        print(f\"Skipping run {run_dir} due to missing data\")\n",
    "                        continue\n",
    "\n",
    "                    self.images.extend(run_images)\n",
    "                    self.labels.extend(labels)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing run {run_dir}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Compute dataset-wide statistics\n",
    "        all_images = torch.cat([torch.tensor(img, dtype=torch.float32).unsqueeze(0) for img in self.images])\n",
    "        self.mean = torch.mean(all_images)\n",
    "        self.std = torch.std(all_images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.tensor(self.images[idx], dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        image = (image - self.mean) / self.std  # Normalize using dataset-wide stats\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd1d4d4-df4e-404c-88f7-880f4bf0a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GridCellCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GridCellCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv3d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv3d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.fc1 = None  # Placeholder for the first fully connected layer\n",
    "        self.dropout = nn.Dropout(0.5)  # Regularization\n",
    "        self.fc2 = nn.Linear(256, 2)  # Predict sine and cosine\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # Dynamically calculate flattened size and define fc1\n",
    "        if self.fc1 is None:\n",
    "            flattened_size = x.view(x.size(0), -1).size(1)\n",
    "            self.fc1 = nn.Linear(flattened_size, 256)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b082e5-6f3c-4d0b-b287-46828c4adac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def angular_loss(y_pred, y_true):\n",
    "    # Normalize predictions and true values to ensure they lie on the unit circle\n",
    "    y_pred = F.normalize(y_pred, p=2, dim=-1)\n",
    "    y_true = F.normalize(y_true, p=2, dim=-1)\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = torch.sum(F.normalize(y_pred, p=2, dim=-1) * F.normalize(y_true, p=2, dim=-1), dim=-1)\n",
    "    return 1 - cosine_similarity.mean() + 0.01 * (y_pred.norm(p=2) + y_true.norm(p=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae48fee-d9be-4b78-9e34-e6cff405c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train_model(model, train_loader, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)  # Predict sin and cos\n",
    "            # Convert labels (angles) to sin and cos\n",
    "            labels_sin_cos = torch.stack([torch.sin(labels), torch.cos(labels)], dim=1)\n",
    "            loss = angular_loss(outputs, labels_sin_cos)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df655f9-feeb-44ee-b836-c62130b94102",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess the VR trajectory data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b77a99e-af28-4aba-9e77-ef84d0ee9881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 6747\n"
     ]
    }
   ],
   "source": [
    "subjects = ['s05', 's14']\n",
    "base_data_dir = r'C:\\Users\\sdabiri\\OneDrive - Georgia Institute of Technology\\BMED 8803 - Stat ML for Neural data\\Project\\preprocessed'\n",
    "base_behavioral_dir = r'C:\\Users\\sdabiri\\OneDrive - Georgia Institute of Technology\\BMED 8803 - Stat ML for Neural data\\Project\\Small_Dataset'\n",
    "time_interval = 1.5\n",
    "\n",
    "# Create the dataset\n",
    "dataset = BrainDataset(subjects, base_data_dir, base_behavioral_dir, time_interval)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Split into training and validation datasets\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39fad9f3-5e82-44c2-88b1-ed0a8b6dc029",
   "metadata": {},
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Define dataset\n",
    "run_dirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) if d.startswith('run')]\n",
    "dataset = BrainDataset(run_dirs, behavioral_dir, time_interval)\n",
    "\n",
    "# Split into training and validation datasets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # Remaining 20% for validation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)  # Shuffle not needed for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f8c9d7-33b2-43c2-8d52-7b8901ede660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomRotation, RandomHorizontalFlip\n",
    "\n",
    "transform = Compose([\n",
    "    RandomRotation(10),  # Rotate up to 15 degrees|\n",
    "    RandomHorizontalFlip(),  # Flip images horizontally\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec708a-0406-4a6e-9b29-4700751ded1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edda29ae-da33-41cc-a149-b7e095a9fbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch shape: torch.Size([4, 1, 96, 96, 20]), Labels: tensor([  0.6849, 240.6274, 299.6090, 317.1288])\n",
      "Validation batch shape: torch.Size([4, 1, 96, 96, 20]), Labels: tensor([265.3368,  65.7651,  46.5793, 344.9536])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(f\"Train batch shape: {images.shape}, Labels: {labels}\")\n",
    "    break\n",
    "\n",
    "for images, labels in val_loader:\n",
    "    print(f\"Validation batch shape: {images.shape}, Labels: {labels}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc58560-1240-4659-af55-6b70655b8d7e",
   "metadata": {},
   "source": [
    "## 3. Set up cross-validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac342d28-5e40-4f1a-94fb-873921d05a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58ae4f5b-e818-4943-bd34-349f6639ccd5",
   "metadata": {},
   "source": [
    "## 4. Build and train the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de4f96e2-3ca5-44ef-90a3-cfb5e88ca900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer\n",
    "model = GridCellCNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1624e2b-5b71-49dc-8bdb-0df995aafffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50  # Arbitrary high value\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "wait = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8a776-0cf5-444d-bc2e-22faaabb78cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Learning Rate: {param_group['lr']}\")\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        labels_sin_cos = torch.stack([torch.sin(labels), torch.cos(labels)], dim=1)\n",
    "        loss = angular_loss(outputs, labels_sin_cos)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        running_loss += loss.item()\n",
    "        predicted_angles = torch.atan2(outputs[:, 0], outputs[:, 1]) * 180 / torch.pi\n",
    "        predicted_angles = predicted_angles % 360  # Ensure angles are in [0, 360)\n",
    "        correct += (torch.abs(predicted_angles - labels) % 360 <= 10).sum().item()  # Accuracy within 10 degrees\n",
    "        total += labels.size(0)\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracies.append(100 * correct / total)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            labels_sin_cos = torch.stack([torch.sin(labels), torch.cos(labels)], dim=1)\n",
    "            loss = angular_loss(outputs, labels_sin_cos)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            # Metrics\n",
    "            predicted_angles = torch.atan2(outputs[:, 0], outputs[:, 1]) * 180 / torch.pi\n",
    "            predicted_angles = predicted_angles % 360\n",
    "            val_correct += (torch.abs(predicted_angles - labels) % 360 <= 10).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    val_losses.append(val_running_loss / len(val_loader))\n",
    "    val_accuracies.append(100 * val_correct / val_total)\n",
    "\n",
    "    # Adjust learning rate\n",
    "    scheduler.step(val_running_loss / len(val_loader))   # Uses validation loss\n",
    "\n",
    "\n",
    "    # Print metrics for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "          f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "    if val_running_loss / len(val_loader) < best_val_loss:\n",
    "        best_val_loss = val_running_loss / len(val_loader)\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save best model\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1}\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea5c7a-6b77-45eb-87b4-77d4f97d22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "835d39b3-d6f2-40a2-8d8a-c20d81f66d58",
   "metadata": {},
   "source": [
    "# External cross-validation (for future use on all subjects)\n",
    "def cross_validate_subjects(subject_ids, data_dir):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, test_idx in kf.split(subject_ids):\n",
    "        train_subjects = [subject_ids[i] for i in train_idx]\n",
    "        test_subjects = [subject_ids[i] for i in test_idx]\n",
    "        print(f\"Train on {train_subjects}, Test on {test_subjects}\")\n",
    "        # Implement training/testing for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b240a-ae79-498c-a003-f01740901db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
